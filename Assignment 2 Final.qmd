---
title: "Assignment 2"
---

```{r}
#| message: false
#| warning: false
library(dplyr)
library(tidyr)
library(knitr)
library(ggplot2)


data <- read.table("Collider_Data_2025.txt", header = TRUE)
```

### Plotting the Coordinates of Each Particle, and Colouring Coding Based on Particle Type

```{r}

class <- factor(ifelse(data$Y1 == 1, "alpha", 
                ifelse(data$Y2 == 1, "beta","rho")))

marker_type <- factor(ifelse(data$X3 == 1, "Type A", "Type B"))
  
Q1_plot <- ggplot() + 
  geom_point(data = data, aes(x= X1, y = X2, color = class, shape=marker_type),size=2) + 
  theme_minimal() +
  labs(
    title = "Feature Space Visualization Colored by Response Variable",
    x     = "X1",
    y     = "X2",
    color = "Particle Type", 
    shape = " Detector Type"
  ) +
  theme(aspect.ratio = 1)

# ggsave(Q1_plot, filename = "Figures/Q1 Scatter Plot.png")
```

If differentiating between alpha and beta particles, a linear decision boundary could have been appropriate. However, differentiating between rho particles and the others would require a nonliner decision boundary. This means that nonlinear machinery is definitely an apporpriate model class for thsi problem. There , both alpha and beta particualrs curve around the rho particles, and there is no straight line that can seperat these curved regions, which suggest that we need nonlinear machinery like a neural network.

### Soft-Max Activation Function in Matrix Form

```{r}

# where Z is a dL x N matrix that contains the z values for each node in the output layer for each observation

Z <- matrix(c(1,2,4,3,5,6,8,7,9,10), nrow = 2)

softmax <- function(Z){ 
  
  Z_exp <- exp(Z)
  A_1 <- Z_exp * t(as.matrix(1/colSums(Z_exp), ncol = nrow(Z_exp), byrow = FALSE) %*% matrix(1,ncol = nrow(Z_exp)))
  
  return(t(A_1))
}

softmax(Z)

```

### Cross Entropy Error Function

```{r}
g <- function(Yhat, Y) {
  true_class_col <- max.col(Y)
  
  rc_pairs <- cbind(seq_len(nrow(Yhat)), true_class_col)

  prob <- Yhat[rc_pairs]
  
  return(-mean(log(prob)))
}
  
Y <- matrix(c(1,0,0,0,1,0,0,0,1), nrow = 3)
Yhat <- matrix(c(0.5,0.2,0.3,0.3,0.5,0.2,0.2,0.3,0.5), nrow = 3)  
g(Yhat,Y)
```

### Forward Pass of an (m,m)AFnetwork

```{r}
# Specify activation functions for the hidden and output layers:
sig1 = function(z) {tanh(z)}  #1/(1+exp(-z))
sig2 = function(z) {tanh(z)}
sig3 = function(z) {tanh(z)}

## Neural Network
neural_net = function(X, Y, theta, m, nu)
{
	 # Relevant dimensional variables:
   N = dim(X)[1]
   p = dim(X)[2]
   q = dim(Y)[2]
   a = p 
   Z = matrix(NA, nrow = q, ncol = N)
   
   # Populate weight-matrix and bias vectors:
   index = 1:(p*a)
   W1    = matrix(theta[index],p,a)
   index = max(index)+1:((p+a)*m)
   W2    = matrix(theta[index],p+a,m)
   index = max(index)+1:(m*m)
   W3    = matrix(theta[index],m,m)
   index = max(index)+1:(m*q)
   W4    = matrix(theta[index],m,q)
   
   index = max(index)+1:a
   b1    = matrix(theta[index],a,1)
   index = max(index)+1:m
   b2    = matrix(theta[index],m,1)
   index = max(index)+1:m
   b3    = matrix(theta[index],m,1)
   index = max(index)+1:q
   b4    = matrix(theta[index],q,1)
  
   # Storage
   Yhat  = matrix(NA,N,q)
   error = rep(NA,N)
   
   # Evaluate network:
   for(i in 1:N)
   {
   	  a0 = matrix(X[i,],p,1)
   	  
   	  z1 = t(W1) %*% a0 + b1
   	  a1 = sig1(z1)
   	  
   	  z2 = t(W2)%*% rbind(a0,a1) + b2
   	  a2 = sig2(z2)
   	  
   	  z3 = t(W3) %*% a2 + b3
   	  a3 = sig3(z3)
   	  
   	  z4 = t(W4) %*% a3 + b4
   	  Z[,i] = z4
   }
   A_L = softmax(Z)
     
   Yhat  = A_L
   error = g(Yhat,Y)

   # Calculate error:
   E1 = error
   E2 = E1+nu*(sum(W1^2) + sum(W2^2) + sum(W3^2) + sum(W4^2))/N 
   # modified objective/penalised objective
   
   # Return predictions and error:
   return(list(Yhat = Yhat, E1 = E1, E2 = E2))
}

X <- as.matrix(data[,1:3])
Y <- as.matrix(data[,4:6])
theta <- runif(75,min = -1,max = 1)
results <- neural_net(X,Y,theta,m = 4, nu = 0)
```

# Cross K Fold Validation

```{r}
library(ggplot2)
set.seed(2025)

cross_validation <- function(k,X,Y,theta){
  n_val <- 10
  nu_val <- exp(seq(-6,2,length = n_val ))
  CV_Errors <- c()
  
  for(i in 1:k){
    errors <- c()
    indices <- 1:nrow(X)
    
    index_train <- sample(indices,0.8*nrow(X),replace = FALSE)
    training_X <- X[index_train,]
    training_Y <- Y[index_train,]
  
    index_test <- indices[-index_train]
    test_X <- X[index_test,]
    test_Y <- Y[index_test,]
    
    for (v in 1:n_val) { 
        print(c(i,v))
        theta <- runif(75,min = -1,max = 1)
        
        params <- optim(theta, fn = \(theta) neural_net(training_X, training_Y, theta, m=4, nu=nu_val[v])$E2, method = "BFGS")

        # params <- optim(theta, function(X, Y, theta, m, nu) {return(neural_net(X, Y, theta, m, nu)$E2)}, X = training_X, Y = training_Y, m = 4, nu = nu_val[v], method = "BFGS")
        
        results <- neural_net(X = test_X,Y = test_Y, theta = params$par, m = 4, nu = nu_val[v])
        errors[v] <- results$E1
       }
    CV_Errors <- cbind(CV_Errors,errors)
  }
  CV_Errors <- rowMeans(CV_Errors)
  result <- cbind("Nu Values" = nu_val, "CV Errors"= CV_Errors)
  return(result)
}

```

```{r}
CV_results <- cross_validation(k=3,X,Y, theta)
colnames(CV_results) <- c("Nu_Values","CV_Error")

optimal_nu_index <- which.min(CV_results[,2])
optimal_nu <- unname(CV_results[optimal_nu_index,1])

ggplot() + geom_line(data = as.data.frame(CV_results), aes(x = Nu_Values, y = CV_Error)) + theme_minimal()+
  geom_vline(xintercept = optimal_nu, linetype = "dashed", color = "red") +
  annotate("text",x=optimal_nu, y=-Inf, label = paste0("Nu Value = ", round(optimal_nu,3)), vjust = -0.8, size = 3, hjust = -0.1, color = "red") +
  labs(
    title = "Three Fold Cross Validation Across\nNu Regularization Parameters",
    x     = "Nu Regularised Values",
    y     = "CV Error"
  ) +
  theme_minimal() +
  theme(aspect.ratio = 1)

# ggsave(filename = "Figures/Q2 Validation Plot.png")  
# save(CV_results, file = "R Data/Q2 CV Errors.RData")
```

### Response Curves

```{r}
res <- 1000
x1_seq <- seq(min(data$X1), max(data$X1), length.out = res)
x2_seq <- seq(min(data$X2), max(data$X2), length.out = res)

# full grid
grid <- expand.grid(
  X1 = x1_seq,
  X2 = x2_seq,
  X3 = c(0,1)
)

# Find the optimal parameters given the optimised regularized parameter using training data
theta_final <- runif(75, -1, 1)
optimal_theta <- optim(theta_final, fn = \(theta) neural_net(X, Y, theta, m=4, nu=optimal_nu)$E2, method = "BFGS")

# Find predicted probabilities using the grid of inputs and dummy y variable for storage
dummy_Y <- matrix(0, nrow = nrow(grid), ncol = 3)
Yhat_final <- neural_net(
  X = as.matrix(grid[, c("X1", "X2", "X3")]),
  Y = dummy_Y,
  theta = optimal_theta$par,  # Use optimal theta from training set
  m = 4,
  nu = optimal_nu
)
probs  <- Yhat_final$Yhat   # matrix nrow(grid) Ã— 3

```

```{r}
# Assign predicted class
grid$pred <- factor(
  apply(probs, 1, which.max),
  levels = 1:3,
  labels = c("alpha","beta","rho")
)

# Split grid by X3 and plot separately
grid0 <- subset(grid, X3 == 0)
grid1 <- subset(grid, X3 == 1)

# Plot for X3 = 0
p0 <- ggplot(grid0, aes(x = X1, y = X2, fill = pred)) +
  geom_tile() +
  labs(
    title    = "Predicted Response Regions By Input Variables\nAnd Detector Type B (X3 = 0)",
    x        = "X1",
    y        = "X2",
    fill     = "Class"
  ) +
  theme_minimal()+
  theme(aspect.ratio = 1)

# Plot for X3 = 1
p1 <- ggplot(grid1, aes(x = X1, y = X2, fill = pred)) +
  geom_tile() +
  labs(
    title    = "Predicted Response Regions By Input Variables\nAnd Detector Type A (X3 = 1)",
    x        = "X1",
    y        = "X2",
    fill     = "Class"
  ) +
  theme_minimal()+
  theme(aspect.ratio = 1)

# display
print(p0)
print(p1)

# ggsave(p0, filename = "Figures/Q3 Response Curve B (X3=0) Plot.png")  
# ggsave(p1, filename = "Figures/Q3 Response Curve A (X3=1) Plot.png")  
# save(optimal_theta, file = "R Data/Q3 Final Optimal Parameters.RData")
# save(Yhat_final, file = "R Data/Q3 Final Y Hat Predictions For Classes.RData")
```
